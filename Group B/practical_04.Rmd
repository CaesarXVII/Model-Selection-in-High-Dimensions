---
title: "Practice 3"
author: "Alexander Maslev, Hanxiong Wang, Minyoung Lee"
date: "2018/ 3/ 5"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### EXERCISE 1 
replicate Exercise 1,2 from practical 3

```{r,echo=FALSE}

n<-1000
p<-5
Mu<-c(2,4,6,8,10) # location vector
rho<-c(0.2,0.5,0.7)

index_sub_choose<-c(1:p)
sub_matrix <- matrix(data = NA,ncol = p,nrow = 2^p-1)
t=0
for(i in 1:5)
{
  index_matrix <- combn(index_sub_choose,i)
  for(j in 1:ncol(index_matrix))
  {
    t <- t+1
    index_sub <- index_matrix[,j]
    sub_matrix[t,c(index_sub)]  <-  1
  }
}

k<-nrow(sub_matrix)


cv <- matrix(data=NA,nrow = k,ncol = 1)
Exact<-data.frame(matrix(0,3,2))
colnames(Exact)<-c("AIC","CV")
rownames(Exact)<-c("r=0.2","r=0.5","r=0.7")

AverageN<-matrix(0,nrow=100,ncol = 2)
colnames(AverageN)<-c("AIC","CV")

Correct<-data.frame(matrix(0,3,2))
colnames(Correct)<-c("AIC","CV")
rownames(Correct)<-c("r=0.2","r=0.5","r=0.7")

Average<-data.frame(matrix(0,3,2))
colnames(Average)<-c("AIC","CV")
rownames(Average)<-c("r=0.2","r=0.5","r=0.7")

  
for(m in 1:3){
for(l in 1:100){
    ### SETTING ###

  sigma<-matrix(0,p,p)
  for(i in 1:p){for(j in 1:p){
    sigma[i,j]<-rho[m]^abs(i-j)}}

  X<-mvrnorm(n , Mu, sigma)
  beta<-c(3,1.5,0,2,0)
  e<-rnorm(n, mean = 0, sd = 1)
  Y_hat<-X%*%beta+e
  
  ### AIC ###    
  RSS<-rep(0,k)
  AIC<-rep(0,k)

  for(j in 1:k){
    Xsub<-X[,which(sub_matrix[j,]==1)]
    lm_AIC<-lm(Y_hat~Xsub)
    new_Y   <-cbind(rep(1,500),Xsub)%*% lm_AIC$coefficients
    for(i in 1:(n/2)){
      RSS[j]<-RSS[j]+(new_Y[i]-Y_hat[i])^2
    }
    AIC[j]<-RSS[j]/var(e)+2*ncol(data.frame(Xsub))
  }
  
  BEST<-sub_matrix[which.min(AIC),]
  BEST[is.na(BEST)] <-0
  if(sum(BEST-c(1,1,0,1,0))==0){
    
    Exact[m,1]<-Exact[m,1]+1
    
  }
  if(sum((BEST[c(1,2,4)]-c(1,1,1)))==0){
    Correct[m,1]<-Correct[m,1]+1
  }
  AverageN[l,1]<-sum(BEST)
  
  
  ### CV ###
  index <- sample(1:n, size=0.5*n)
  y_train<- Y_hat[-index,]
  x_train<-X[-index,]
  y_test<- Y_hat[index,]
  x_test<-X[index,]
  
  for(j in 1:k){
    Xsub    <-x_train[,which(sub_matrix[j,]==1)]
    lm_CV <-lm(y_train~Xsub)
    new_Y   <-cbind(rep(1,500),x_test[,which(sub_matrix[j,]==1)])%*% lm_CV$coefficients
    cv[j,]      <- t(y_test-new_Y)%*%(y_test-new_Y)
    
  }
  
  BEST_cv<-sub_matrix[which.min(cv),]
  BEST_cv[is.na(BEST_cv)] <-0
  if(sum(BEST_cv-c(1,1,0,1,0))==0){
    
    Exact[m,2]<-Exact[m,2]+1
    
  }
  if(sum((BEST_cv[c(1,2,4)]-c(1,1,1)))==0){
    Correct[m,2]<-Correct[m,2]+1
  }
  AverageN[l,2]<-sum(BEST_cv)
  
}
Average[m,]<-colMeans(AverageN)
}

Exact/100
Correct/100
Average

```

We have similar values of Exact and Correct and CV regardless of rho. We have AIC better than CV. 

### EXERCISE 3

#### a)
Firs we input the data and use "glm" to build the general linear model. Because we need to use that to predict the data set
```{r}
## load the data  ##
setwd("D:/Gen?ve Universit?/High Dimention Data/practical_4")
load("data_leukemia_reduced.Rda")
data <- data_leukemia_reduced
str(data)
plot(data)

## a ##
## use glm to make a regression ##
glm_1 <- glm(formula = y ~ V980 ,family = binomial,data = data )
summary(glm_1)

```

#### b)
In this pare we need to modify the type of data. And create a contingency tables and shows [1,1] is true positive; [0,1] is false positive; [0,0] is false negative ;[1,0] is the true negative.The table is in t. And in the end we get tp (Predict positive and the real value is positive ), np (. Predict positive and the real value is negative ).
```{r}
## b ##
data_pre <- data.frame(data$V980)
colnames(data_pre)<- "V980"
classify_pre_number <- predict(object = glm_1,newdata = data_pre,type = "response" )
classify_pre_number <- as.data.frame(classify_pre_number)
#Set the cut-of value v980<0.5 is "1"

# here we choose c is euqal to 0.5
c <- 0.5
#Create an empaty matrix to store the data
classify_pre    <-matrix(data = NA,nrow = 72,ncol = 1)
classify_pre[c(which(classify_pre_number<=c)),1]<-0 
classify_pre[c(which(classify_pre_number>c)),1]<-1

comp_pre_rea <- cbind(data[,1],classify_pre)
comp_pre_rea <- data.frame(comp_pre_rea)

colnames(comp_pre_rea) <- c("real","predict")

#
t<-table(comp_pre_rea)
tp[i,1] <- t[1,1]/25
np[i,1] <- t[2,1]/47

```


#### c)
In these part we try different c and create the curve the c we choose is from 0.01 to 1 and each step 0.1, after get these values we can create the curve

```{r}
#### c ####
## creat the curve ##

tp <- matrix(data = NA,nrow = 100,ncol = 1)
fp <- matrix(data = NA,nrow = 100,ncol = 1)

for(i in 1:100){
  c <- i*0.01
  
  
  #Create an empaty matrix to store the data
  classify_pre    <-matrix(data = NA,nrow = 72,ncol = 1)
  classify_pre[c(which(classify_pre_number<=c)),1]<-0 
  classify_pre[c(which(classify_pre_number>c)),1]<-1
  
  comp_pre_rea <- cbind(data[,1],classify_pre)
  comp_pre_rea <- data.frame(comp_pre_rea)
  
  colnames(comp_pre_rea) <- c("real","predict")
  
  t<-table(comp_pre_rea)
  tp[i,1] <- t[1,1]/25
  fp[i,1] <- t[2,1]/47
  
}
## result of the curve ##
plot(x = fp,y = tp )

```


#### d)
In this part we can compare the curve between the curve we build and from the package we get. By comparing the two plot we find these are similar.
```{r}
## d ##
library("pROC", lib.loc="~/R/win-library/3.3")
response <- comp_pre_rea[,1]
predictor <- a
rocobj <- roc(response, predictor) # Very slow!

fp_pack <- 1-rocobj$sensitivities
tp_pack <- rocobj$specificities

#plot from the package
plot(x = fp_pack,y = tp_pack)

```

plot we build

```{r}
plot(x = fp,y = tp )
```



