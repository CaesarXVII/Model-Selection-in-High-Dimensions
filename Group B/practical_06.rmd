---
title: "Practical_6"
author: "Alexander Maslev, Hanxiong Wang, Minyoung Lee"
date: "2018?? 4?? 9??"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## EXERCISE 1

### a)
First, we load the SIS package and the data set.
```{r, warning=FALSE,echo=FALSE,results=FALSE}
rm(list=ls())
require(MASS)
set.seed(11)
library(SIS)
load("data_leukemia_reduced.Rda")
leuk_dat=data_leukemia_reduced
```
We create the X matrix and the vector y of responses.
```{r}
X_leuk=as.matrix(leuk_dat[,2:12])
y_leuk = leuk_dat[,1]
```

### b)
We split the data in half into a train set and a test set.
```{r}
index = sample(1:length(y_leuk),size = length(y_leuk)/2,replace = F)
train_X = X_leuk[index,]
train_y = y_leuk[index]

test_X = X_leuk[-index,]
test_y = y_leuk[-index]
```

### c)
We create 3 models using 3 possible combinations of penalized methods and tuning constants; the selected covariates and their coefficients are shown below.
```{r warning=FALSE}
mod11=SIS(train_X,train_y,family="binomial",penalty = "SCAD",tune="bic")
mod12=SIS(train_X,train_y,family="binomial",penalty = "SCAD",tune="aic")
mod13=SIS(train_X,train_y,family="binomial",penalty = "SCAD",tune="cv")
mod21=SIS(train_X,train_y,family="binomial",penalty = "MCP",tune="bic")
mod22=SIS(train_X,train_y,family="binomial",penalty = "MCP",tune="aic")
mod23=SIS(train_X,train_y,family="binomial",penalty = "MCP",tune="cv")
mod31=SIS(train_X,train_y,family="binomial",penalty = "lasso",tune="bic")
mod32=SIS(train_X,train_y,family="binomial",penalty = "lasso",tune="aic")
mod33=SIS(train_X,train_y,family="binomial",penalty = "lasso",tune="cv")

mod11$coef.est
mod11$coef.est
mod11$coef.est
mod21$coef.est
mod22$coef.est
mod23$coef.est
mod31$coef.est
mod32$coef.est
mod33$coef.est

```
Models 1 and 2 select identical covariates, while model 3 has an additional variable. We use the models to get the predictions and test the error rate of each model.

```{r}
pred11=predict(mod11,test_X,type="class")
pred12=predict(mod12,test_X,type="class")
pred13=predict(mod13,test_X,type="class")

pred21=predict(mod21,test_X,type="class")
pred22=predict(mod22,test_X,type="class")
pred23=predict(mod23,test_X,type="class")

pred31=predict(mod31,test_X,type="class")
pred32=predict(mod32,test_X,type="class")
pred33=predict(mod33,test_X,type="class")

pred<-matrix(c(sum(pred11!=test_y)/36,sum(pred12!=test_y)/36,sum(pred13!=test_y)/36,
               sum(pred21!=test_y)/36,sum(pred22!=test_y)/36,sum(pred23!=test_y)/36,
               sum(pred31!=test_y)/36,sum(pred32!=test_y)/36,sum(pred33!=test_y)/36),3,3)
colnames(pred)<-c("SCAD","MCP","Lasso")
rownames(pred)<-c("bic","aic","cv")
pred
```
All methods perform equally well in terms of prediction error. 

## EXERCISE 2
### PC Simple Algorithm
```{r, warning=FALSE,echo=FALSE,results=FALSE}
# Basic setting
library(MASS)
set.seed(11)
p<-10
rho<-0.5
n<-1000
sigma<-matrix(0,p,p)
for(i in 1:p){for(j in 1:p){
  sigma[i,j]<-rho^abs(i-j)}}

Mu<-rep(0,10) # location vector
X<-mvrnorm(n , Mu, sigma)
colnames(X)<-c("X1","X2","X3","X4","X5","X6","X7","X8","X9","X10")
beta<-c(3,1.5,0,2,rep(0,6))
e<-rnorm(n, mean = 0, sd = 1)
Y_hat<-X%*%beta+e

```

### a) First variable to enter the algorithm based on the correlation(Y and Xs)
We can not compute partial correlation for the first stage because we don't have variable to conditioning on. Thus We take correlation to select the first variable to enter the model. 
```{r}
var_ind<-which.max(cor(Y_hat,X)) 
var_ind # initial setting

var_name<-colnames(X)[which.max(cor(Y_hat,X))]
var_name 
```
The variable that we will choose for the first variable to enter is X1 because it has highest correlation with Y.

### b) Partial correlation of order 1. 
```{r}

Mm<-X[,-var_ind]
Xc<-X[,var_ind]

par_cor<-rep(0,ncol(Mm))
for(j in 1:ncol(Mm)){
  H<-Xc%*%solve(t(Xc)%*%Xc)%*%t(Xc)
  e<-(diag(n)-H)%*%Y_hat
  I<-diag(n)
  par_cor[j]<-t(e)%*%(I-H)%*%Mm[,j]/sqrt((t(e)%*%e)%*%(t(Mm[,j]%*%(I-H)%*%Mm[,j])))
}

var_name<-c(var_name,colnames(Mm)[which.max(par_cor)])

Fisher_z<-1/2*log((1+par_cor[which.max(par_cor)])/(1-par_cor[which.max(par_cor)]))
sqrt(n-1-3)*abs(Fisher_z)>qnorm(1-0.05/2)

colnames(Mm)[which.max(par_cor)]


```
We compute the partial correlation given that we choose the variable X1. We compute partial correlation of order 1 and did hypothesis test to check the inclusion of the variable. Ho : $\rho_{X_{j}Y,X_{c}} = 0$ is rejected if the logic is TRUE. Based on the hypothesis test, we include X4 as the second variable to the model.

### c) partial correlations of higher order until converge.
```{r}
var_name<-colnames(X)[which.max(cor(Y_hat,X))]
var_name 
var_ind<-which.max(cor(Y_hat,X))
Mm<-X[,-var_ind]
Xc<-X[,var_ind]
for (p in 2:ncol(X)){
  
  par_cor<-rep(0,ncol(Mm))
  for(j in 1:ncol(Mm)){
    H<-Xc%*%solve(t(Xc)%*%Xc)%*%t(Xc)
    e<-(diag(n)-H)%*%Y_hat
    I<-diag(n)
    par_cor[j]<-t(e)%*%(I-H)%*%Mm[,j]/sqrt((t(e)%*%e)%*%(t(Mm[,j]%*%(I-H)%*%Mm[,j])))
    Fisher_z<-1/2*log((1+par_cor[j])/(1-par_cor[j]))
    Hypo_test[j]<-sqrt(n-9-3)*abs(Fisher_z)
  }
  Fisher_z<-1/2*log((1+par_cor[which.max(par_cor)])/(1-par_cor[which.max(par_cor)]))
  
  
  if (sqrt(n-p-3)*abs(Fisher_z)<=qnorm(1-0.05/2)) break 
  
  var_ind<-which.max(par_cor)
  var_name<-c(var_name,colnames(Mm)[which.max(par_cor)])
  Mm<-Mm[,-var_ind]
  Xc<-cbind(Xc,Mm[,var_ind])
            
}
p
var_name

```
We implement the algorithm that will find variables that we choose and the "m" which will meet the stopping condition : $M_{m-1}=M_{m}$. We stop when m = 4 and we select the variables (X1, X4, X2). This is exact model as we have $\beta = [3 1.5 0 2 rep(0,6)].  Even, the order of entering the model is same as the order of the size of the coefficient. 

## EXERCISE 3
### Classification Tree
We need to ge the library and load the data here is the way we lode the data.
To use the function tree grow we inport the library "repart"
```{r, warning=FALSE,echo=FALSE,results=FALSE}

library("rpart", lib.loc="C:/Program Files/R/R-3.4.4/library")
data <- iris
n <- nrow(data)
index <- sample(1:n, size=0.6*n)

```
### a) Split data
Here we split the data randomly and 1/3 fro test and 2/3 for the train data. After that we build the model b using rpart function.and the plot is below
```{r}

data_train<- data[index,]
data_test<- data[-index,]
str(data_train)

```

### b) fit the model
```{r}
rpart.control(minsplit = 20)
model <- rpart(formula = Species~.,data = data_train,minbucket = 1)
plot(model)
plot(model, uniform = TRUE,compress = TRUE,margin = 0.11)
text(model, splits = TRUE)

```

### c)
- tree prune
Here we need to prune the tree for better prediction and model.And here we use the prune the tree by using the "prune" function and the result is showing below.
```{r}

# tree prune
tree_prune <- prune(tree = model,cp = 0.1)
plot(tree_prune,uniform = TRUE,compress = TRUE,margin = 0.3)
text(model, splits = TRUE)


```

- Predict
Here we use the model we build to make a prediction and we can get a out of simple error by using the test data.And here is the test error rate below.
```{r}
# Predict
pred <- predict(object = model,newdata = data_test,type = "class")
real <- data_test[,5]
# How many wrong 
err_pred <- which(pred!=real)
```

### Regression Tree with Zambia data
Similar with the exercise before we deal with the Data Zambia. And follow the similar way to do that.

### a) Split data
Here we split the data randomly and 1/3 fro test and 2/3 for the train data. After that we build the model by using rpart function.
```{r}
load("malnutrion_zambia_cleaned.Rda")
data_zambia <- data_zambia[,c(1:7,21,22,24)]
n_2 <- nrow(data_zambia)
index <- sample(1:n_2, size=0.6*n_2)
data_train_2<- data_zambia[index,]
data_test_2<- data_zambia[-index,]
str(data_train_2)
```

### b) fit the model
```{r}
model.zb <- rpart(formula = `Height for age sd`~.,data = data_train_2,minbucket = 1)
plot(model.zb)
plot(model.zb, uniform = TRUE,compress = TRUE,margin = 0.11)
text(model.zb, splits = TRUE,cex = 0.8)

```

### c)
- tree prune
Here we need to prune the tree for better prediction and model.And here we use the prune then before we prune the tree
```{r}

tree_prune_2 <- prune(tree = model.zb,cp = 0.1)
plot(tree_prune_2,uniform = TRUE,compress = TRUE,margin = 0.3)
text(model.zb, splits = TRUE)


```

- Predict
Here we use the model we build to make a prediction and we can get a out of simple error by using the test data.And here is the test error rate below.
Because the y is a continue variable so we only calculate the rss to see how good the model is.
```{r}
# Predict
pred_2 <- predict(object = model.zb,newdata = data_test_2)
real <- data_test_2[,1]
d    <- as.matrix(real-pred_2)
rss <- t(d)%*%d

```



