---
title: "Practical 3"
author: "group members names"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Simulations and CV

In this exercise we would like to program k-fold Cross-Validation (with k=2) and do model
selection in a specific simulation setting with an exhaustive search.


  a) Generate from a MVN (multivariate normal) a matrix $\ X_{n \times p}$ with n=1000 and p=5.
     You can choose the location vector as you wish but set the scale matrix as the identity.
     
```{r, eval = T }
# Clear objects
rm(list = ls())
# Clear console
cat("\014")
set.seed(1)
library(MASS)
media<-rep(0,5) # (standard MVN)
I<-diag(1,5)
X<-mvrnorm(n = 1000, mu=media, Sigma=I);


# n:	the number of samples required.
# mu: a vector giving the means of the variables.
# Sigma:	a positive-definite symmetric matrix specifying the covariance matrix of the variables.
# tol: tolerance (relative to largest variance) for numerical lack of positive-definiteness in Sigma.
# empirical: logical. If true, mu and Sigma specify the empirical not population mean and covariance matrix.
# EISPACK: logical, values other than FALSE are an error.

```

  b) Choose the generating vector $\beta$ = [3 1.5 0 2 0] and retrieve the signal to noise ratio of
     this setting.

```{r, eval =T}
beta <-c(3,1.5,0,2,0) # true beta (population parameter)
mu <-X%*%beta
stand.dev <-1
SNR <-norm(mu,type = "2")/stand.dev;

```

The signal to noise ratio of this setting is $125.64$. See <https://arxiv.org/abs/1204.6054> for more details.

  c) Generate $\hat{y}$ thanks to the relation  $y = \ X_{n \times p}\beta + \epsilon$ where $\epsilon_i$
     is a standard normal, and n=1000 and p=5.
     Suppose for simplicity that the errors are uncorrelated.

```{r, eval=TRUE}

# Noise
epsilon <-rnorm(n = 1000, mean=0, sd=1)
epsilon <-as.matrix(epsilon)

# True y using the true beta  
beta <-as.matrix(beta)
y <-X %*%beta + epsilon

# y.hat using lm function: my new beta.hat is the ols beta: beta.hat= (X'X)^(-1)X'y
mod.lin <-lm(y~X)
coeff.est <-as.vector(mod.lin$coefficients); # coeff.est ; Don't consider beta0
beta.hat <-c(coeff.est[2],coeff.est[3],coeff.est[4],coeff.est[5],coeff.est[6])
y.hat <-as.vector(mod.lin$fitted.values)

```

 d) Split the data randomly in two halves (k=2) and use the training set to determine
    $\hat{\beta_{mle}}$. Then, making use of the specific loss function of the linear regression, 
    calculate the test set cross validation score for each possible model. Conclude on the best model.

```{r, eval=T}
library(utils)
k <-2
n <-nrow(X)
tot.obs <-seq(from=1, to=n, by=1)
obs.sample<-sample(tot.obs, size=round(n/k))
X.train <-X[obs.sample,]
y.train <-y[obs.sample]
X.test <-X[-obs.sample,]
y.test <-y.hat[-obs.sample]

all.possible.regressions <- function(dat, k){
    n <- nrow(dat)
    regressors <- paste("x", 1:k, sep="")
    lst <- rep(list(c(T, F)), k)
    regMat <- expand.grid(lst);
    names(regMat) <- regressors
    formular <- apply(regMat, 1, function(x)
            as.character(paste(c("y ~ 1", regressors[x]), collapse="+")))
    allModelsList <- apply(regMat, 1, function(x)
            as.formula(paste(c("y ~ 1", regressors[x]),collapse=" + ")) )
    allModelsResults <- lapply(allModelsList,
            function(x, data) lm(x, data=data), data=dat)
    n.models <- length(allModelsResults)
    extract <- function(fit) {
        err <- norm((fit$fitted.values - y.test), type = "2")
        df.sse <- fit$df.residual
        p <- n - df.sse -1
        sigma <- summary(fit)$sigma
        MSE <- sigma^2
        R2 <- summary(fit)$r.squared
        R2.adj <- summary(fit)$adj.r.squared
        sse <- MSE*df.sse
        aic <- n*log(sse) + 2*(p+2)
        bic <- n*log(sse) + log(n)*(p+2)
        out <- data.frame(err = err, df.sse=df.sse, p=p, SSE=sse, MSE=MSE,
            R2=R2, R2.adj=R2.adj, AIC=aic, BIC=bic)
        return(out)
    }
    result <- lapply(allModelsResults, extract)
    result <- as.data.frame(matrix(unlist(result), nrow=n.models, byrow=T))
    result <- cbind(formular, result)
rownames(result) <- NULL
colnames(result) <- c("model", "err", "df.sse", "p", "SSE", "MSE", "R2",
"R2.adj", "AIC", "BIC")
    return(result)
}

x1 <- X[,1]
x2 <- X[,2]
x3 <- X[,3]
x4 <- X[,4]
x5 <- X[,5]
data <- as.data.frame(cbind(y,X))

model.selec <- all.possible.regressions(dat=data, k=5)
model.selec
which(model.selec$err == min(model.selec$err))
```

Source : <https://stat.ethz.ch/pipermail/r-help/2010-May/239617.html>


We split the data randomly and then we set the seed to retain the results.
According to the loss function criteria $$||\hat{y}- y||_2$$ , the model that contains the intercept  and $X_3$ is the best in the sense of that criteria.


 e) Suppose now that we increase the size of $\beta$ to 100 (i.e. $p=100$). Calculate the number
    of possible models together with an estimate of the time needed for an exhaustive
    search (hint: use previous results). Conclude on the feasibility of the task

In order to calculate the number of possible models to evaluate, it's necessary to recall
the binomial theorem: $\sum_{k=0}^{n}\binom{n}{k} = 2^n$ .  
We don't consider k=0 (model without any predictor) so we have $\ 2^p - 1$ . 
With p=100 we have $\ 2^p - 1 =   1.267651*10^{30}$ models to evaluate. 
Obviously the time used will be very huge!
Anyway one can time the evaluation of an R expression using system.time function.

If the size of $\beta$ ($=p$) is 100, then it stays smaller than $n$. So the task is mathematically feasible. However since $p$ is large, the task is numerically unfeasible since the sofware will generate random numbers for the estimates.


***


### Simulation and AIC

 In this exercise we would like to repeat the steps underlined above in the specific case of the
 Akaike information criterion.

   a)  Retrieve the values found up to point c) of exercise 1.

```{r, eval = T}
model.selec <- all.possible.regressions(dat=data, k=5)
```

   b)  Calculate the AIC for all possible models when p=5 without using the predefined
        function present in R. Conclude on the best model.

```{r eval=T}
which(model.selec$AIC == min(model.selec$AIC))

```
According to the AIC criterion, the model with the intercept, $X_1$, $X_2$ and $X_4$ is the best.


   c)  As for the previous exercise, suppose now that we increase the size of $\beta$ to 100 
       (i.e. p=100). Conclude on the feasibility of the task.

If the size of $\beta$ ($=p$) is 100, then it stays smaller than $n$. So the task is mathematically feasible. However since $p$ is large, the task is numerically unfeasible since the sofware will generate random numbers for the estimates.


   d)   Compare the performance of CV and AIC by replicating 100 times the tasks of Exercise
        1 and 2. In particular you should evaluate three specific criteria: the proportion
        of times the correct model is selected (Exact), the proportion of times the selected
        model contains the correct one (Correct) and the average number of selected regressors
        (Average #]).


***

### Real data application

 Load the Zambia dataset and perform an exhaustive search on the continuous covariates
 (i.e. avoiding factors) based on CV and AIC in order to find the best model.
 
 You can either employ your codes derived in Exercise 1 and 2 or make use of the existing R
 packages: leaps, glmulti, MuMIn and caret.

```{r eval=F }

require(foreign)  # install foreign package if you do not have it yet


##### See section 1.6.2 e-book for information on the dataset. ####

# dat = read.spss("Zambia.SAV", add.undeclared.levels = "no")

dat = read.spss("Zambia.SAV")

# Construct system matrix

# The idea behind this exercise is to be aware that data cleaning is most of the times the real issue 
# with a real problem. It is sensitive to say that 80% of the work is cleaning and only 20% is modeling.

# Extract response variable i.e. HW70 Height for age standard deviation (according to WHO)
y = dat$HW70
y[y == 9996] = NA
y[y == 9997] = NA
y[y == 9998] = NA
y[y == 9999] = NA

# Revert tranformation (i.e. z-score)
y = as.numeric(y)/100

# Variable 1: The calculated months of breastfeeding gives the duration of breastfeeding
x1 = dat$M5
x1[x1 == 94] = 0
x1[x1 == 97] = NA
x1[x1 == 98] = NA
x1[x1 == 99] = NA
x1[as.numeric(x1) > 40] = NA

# Variable 2: Age in months of the child
x2 = dat$HW1

# Variable 3: Age of the mother at birth
x3 = dat$V012 - dat$B8
x3[x3>45] = NA

# Variable 4: Body mass index (BMI) of the mother
x4 = dat$V445

x4 = as.numeric(x4)/100  # no sense without this division

# Variable 5: Height of the mother in meters
x5 = dat$V438
x5[x5 == 9998] = NA
x5[x5 == 9999] = NA
x5[x5 < 1300] = NA
x5[x5 > 1900] = NA

x5 = x5/1000  # it was in mm, we need to transform from original

# Variable 6: Weight of the mother in kilograms
x6 = dat$V437

x6=x6/10 # we need to go back to Kg

# Variable 7: De facto region of residence

# Creating dummies (i.e. indicator functions) for each level of an existing factor enables
# to check the coefficients of each level in a possible future model estimation

x7 = as.factor(dat$V101)


x7 = model.matrix(~x7-1)

dim(x7)

# Variable 8: Mother highest education level attended
x8 = as.factor(dat$V106)
x8 = model.matrix(~x8-1)

dim(x8)

# Variable 9: Wealth index factor score
x9 = dat$V191

# Variable 10: Weight of child at birth given in kilograms with three implied decimal places
x10 = dat$M19
x10[x10 == 9996] = NA
x10[x10 == 9997] = NA
x10[x10 == 9998] = NA
x10[x10 == 9999] = NA
x10 = as.numeric(x10)/1000

# Variable 11: Child Sex
x11 = dat$B4

# Variable 12: Preceding birth interval is calculated as the difference in months between 
# the current birth and the previous birth

x12 = dat$B11
x12[x12 > 125] = NA

# Variable 13: Drinking Water
x13 = dat$V113
x13 = model.matrix(~x13-1)
x13 = x13[,c(2,3,4,8,9,13,17,18)]

dim(x13)

levels(x13)

mat.sys = na.omit(cbind(y,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13))
dim(mat.sys)[2]


# Number of regressor
p = dim(mat.sys)[2]

# Construct X and Y
y = mat.sys[,1]
X = mat.sys[,2:p]

# Create a dataframe

data_zambia = cbind(y,X)

data_zambia = data.frame(data_zambia)



## Part b: Associate proper names to each variable (hint: look at the comments in the r chunk).

View(X)

colnames(data_zambia) = c("Height for age sd", "Breastfeeding duration (months)","Age of the child (months)", "Age of the mother (years)", "BMI mother", "Heigth mother (meter)", "Weight mother (kg)", "Region:Central", "Region:Copperbelt", "Region:Eastern", "Region:Luapula", "Region:Lusaka", "Region:Northern", "Region:Northwestern", "Region:Southern", "Region:Western", "Ed:No education", "Ed:Primary", "Ed:Secondary", "Ed:Higher", "Wealth index factor score", "Child weight at birth (kg)", "Child sex", "Interval between births","Water:Piped into dwelling", "Water:Piped to yard/plot", "Water:Public tap/standpipe", "Water:Protected well", "Water:Unprotected well", "Water:River/dam/lake/ponds/stream/canal/irrigation channel", "Water:Bottled water", "Water:Other")

View(data_zambia)

require("MuMIn")

```

For more information about the libraries, I found on the net:

  + **leaps**:[leaps](https://cran.r-project.org/web/packages/leaps/leaps.pdf)
    - description: leaps() performs an exhaustive search for the best subsets of the variables in x for 
      predicting y in linear regression, using an efficient branch-and-bound algorithm. 
      It is a compatibility wrapper for regsubsets does the same thing better.
      Since the algorithm returns a best model of each size, the results do not depend on a penalty model 
      for model size: it doesn't make any difference whether you want to use AIC, BIC, CIC, DIC, ...


  + **glmulti**:[glmulti](https://cran.r-project.org/web/packages/glmulti/glmulti.pdf)
    - descrition: Automated model selection and model-averaging. Provides a wrapper for glm and other               functions, automatically generating all possible models (under constraints set by the user) with
      the specified response and explanatory variables, and finding the best models in terms of some 
      Information Criterion (AIC,AICc or BIC). Can handle very large numbers of candidate models.
      Features a Genetic Algorithm to find the best models when an exhaustive screening of the candidates 
      is not feasible.


  + **MUMIn**:[MUMIn](https://cran.r-project.org/web/packages/MuMIn/MuMIn.pdf)
    - description: Tools for performing model selection and model averaging. Automated model selection through
      subsetting the maximum model, with optional constraints for model inclusion. 
      Model parameter and prediction averaging based on model weights derived from information criteria
     (AICc and alike) or custom model weighting schemes.


  + **caret**:[caret](https://cran.r-project.org/web/packages/caret/caret.pdf)
    - description: Misc functions for training and plotting classification and regression models.

