---
title: "Practical 3"
author: "group members names"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Simulations and CV

In this exercise we would like to program k-fold Cross-Validation (with k=2) and do model
selection in a specific simulation setting with an exhaustive search.


  a) Generate from a MVN (multivariate normal) a matrix $\ X_{np}$ with n=1000 and p=5.
     You can choose the location vector as you wish but set the scale matrix as the identity.
     
```{r }
library(MASS)
media<-rep(0,5) # mean of the MVN (location vector... I chose a standard MVN)
I<-diag(1,5)
X<-mvrnorm(n = 1000, mu=media, Sigma=I); #dim(X)
#View(X)

# n:	the number of samples required.
# mu: a vector giving the means of the variables.
# Sigma:	a positive-definite symmetric matrix specifying the covariance matrix of the variables.
# tol: tolerance (relative to largest variance) for numerical lack of positive-definiteness in Sigma.
# empirical: logical. If true, mu and Sigma specify the empirical not population mean and covariance matrix.
# EISPACK: logical, values other than FALSE are an error.

```

  b) Choose the generating vector $\beta$ = [3 1.5 0 2 0] and retrieve the signal to noise ratio of
     this setting.

```{r }
beta<-c(3,1.5,0,2,0) # true beta (population parameter)
# I suppose, surfing the net that SNR is X*beta (explained part of the model) divided by the 
# standard deviation of the noise (sigma^2, in point 3) sigma=1)
mu<-X%*%beta
stand.dev<-1
SNR=mu/stand.dev; #dim(SNR)

```

  c) Generate $\hat{y}$ thanks to the relation  $y = \ X_{np}\beta + \epsilon$ where $\epsilon_i$
     is a standard normal, and n=1000 and p=5.
     Suppose for simplicity that the errors are uncorrelated.

```{r }

# I calculate the noise
epsilon<-rnorm(n = 1000, mean=0, sd=1)
epsilon<-as.matrix(epsilon)

# I calculate the true y using the true beta  
beta<-as.matrix(beta)
#dim(beta)
#dim(epsilon)
y<-X %*%beta + epsilon

# I calculate the y.hat using lm function: my new beta.hat is the ols beta: beta.hat= (X'X)^(-1)X'y
mod.lin<-lm(y~X)
coeff.est<-as.vector(mod.lin$coefficients); # coeff.est # I don't consider beta0 
beta.hat<-c(coeff.est[2],coeff.est[3],coeff.est[4],coeff.est[5],coeff.est[6])
fitted<-as.vector(mod.lin$fitted.values)

```

 d) Split the data randomly in two halves (k=2) and use the training set to determine
    $\hat{\beta_{mle}}$. Then, making use of the specific loss function of the linear regression, 
    calculate the test set cross validation score for each possible model. Conclude on the best model.

```{r, eval=F}
library(utils)
ma<-NULL
k<-2
tot.obs<-seq(from=1, to=1000, by=1)
n<-nrow(X)
obs.sample<-sample(tot.obs, size=n/k)
X.train<-X[obs.sample,]
y.train<-y[obs.sample]
X.test<-X[-obs.sample,]
y.test<-y.hat[-obs.sample]

for(i in 1:5){
  indice<-combn(5, i)
  beta.fit<-as.vector(lm(y.train~X.train[,c(indice)])$coefficients)
  cross<-mean( (beta - beta.fit)^2 )
  ma<-c(ma,cross)
}
View(ma)

```

 e) Suppose now that we increase the size of $\beta$ to 100 (i.e. p=100). Calculate the number
    of possible models together with an estimate of the time needed for an exhaustive
    search (hint: use previous results). Conclude on the feasibility of the task


In order to calculate the number of possible models to evaluate, it's necessary to recall
the binomial theorem: $\sum_{k=0}^{n}\binom{n}{k} = 2^n$ .  
We don't consider k=0 (model without any predictor) so we have $\ 2^p - 1$ . 
With p=100 we have $\ 2^p - 1 =   1.267651*10^{30}$ models to evaluate. 
Obviously the time used will be very huge!
Anyway one can time the evaluation of an R expression using system.time function.


***


### Simulation and AIC

 In this exercise we would like to repeat the steps underlined above in the specific case of the
 Akaike information criterion.

   a)  Retrieve the values found up to point c) of exercise 1.

```{r }
library(MASS)
media<-rep(0,5) 
I<-diag(1,5)
X<-mvrnorm(n = 1000, mu=media, Sigma=I); 
beta<-c(3,1.5,0,2,0)                       # true beta (population parameter)
epsilon<-rnorm(n = 1000, mean=0, sd=1)
epsilon<-as.matrix(epsilon)
beta<-as.matrix(beta)
y<-X %*%beta + epsilon
mod.lin<-lm(y~X)
coeff.est<-as.vector(mod.lin$coefficients); 
fitted<-as.vector(mod.lin$fitted.values)


```

   b)  Calculate the AIC for all possible models when p=5 without using the predefined
        function present in R. Conclude on the best model.

```{r eval=F}
library(utils)
matx<-NULL
for(i in 1:5){
  indice<-combn(5, i)
  mod<-lm(y~X[,c(indice)])
  akaike<-AIC(mod)
  matx<-c(matx,akaike)
}
best.AIC<-min(matx)

```

   c)  As for the previous exercise, suppose now that we increase the size of $\beta$ to 100 
       (i.e. p=100). Conclude on the feasibility of the task.

I think that the answer is the same... also in this case we should have $\ 2^p-1$ models with p=100
I don't think it would be a feasible task.


   d)   Compare the performance of CV and AIC by replicating 100 times the tasks of Exercise
        1 and 2. In particular you should evaluate three specific criteria: the proportion
        of times the correct model is selected (Exact), the proportion of times the selected
        model contains the correct one (Correct) and the average number of selected regressors
        (Average #]).

```{r eval=FALSE}
times<-100
library(utils)
ma<-NULL
matx<-NULL
k<-2

for(k in 1:times){

for(i in 1:5){
  indice<-combn(5, i)
  beta.fit<-as.vector(lm(y.train~X.train[,c(indice)])$coefficients)
  cross<-mean( (beta - beta.fit)^2 )
  ma<-c(ma,cross)
  akaike<-AIC(mod)
  matx<-c(matx,akaike)
}
  
}

```


***

### Real data application

 Load the Zambia dataset and perform an exhaustive search on the continuous covariates
 (i.e. avoiding factors) based on CV and AIC in order to find the best model.
 
 You can either employ your codes derived in Exercise 1 and 2 or make use of the existing R
 packages: leaps, glmulti, MuMIn and caret.

```{r eval=F }

# I think that the full model is the one with all 13 covariets 
# (I don't think the model with 948= dim(Zambia)[2] - 1 covariates).
# now, using Cesare codes, I try to build the full model with 13 covariets

require(foreign) # for read.spss
library(haven)   # for read_sav
dat = read.spss("C:/Users/maria.zoia.UNICATT/Desktop/1) cartella non condivisa giulia/0) magistrale Ginevra/2) corsi ginevra/2) insegnamenti/1) 1 anno/2) semestre/4) Big Data/2) esercizi e-book/1) capitolo 1/Zambia.SAV")  
Zambia<-read_sav("C:/Users/maria.zoia.UNICATT/Desktop/1) cartella non condivisa giulia/0) magistrale Ginevra/2) corsi ginevra/2) insegnamenti/1) 1 anno/2) semestre/4) Big Data/2) esercizi e-book/1) capitolo 1/Zambia.SAV")

# View(dat)
# View(Zambia)
# The two dataframes don't look much alike..

# RESPONSE VARIABLE: 
# HW70: Height for age standard deviation 
y = dat$HW70
y[y == 9996] = NA
y[y == 9997] = NA
y[y == 9998] = NA
y[y == 9999] = NA
# Revert tranformation (i.e. z-score)
y = y/100                                             # I have problems with y
# I try to do it using Zambia
yz= Zambia$HW70
yz[yz== 9996] = NA
yz[yz== 9997] = NA
yz[yz== 9998] = NA
yz[yz== 9999] = NA
# Revert tranformation (i.e. z-score)
yz= yz/100

# COVARIETS:
# Variable 1: The calculated months of breastfeeding gives the duration of breastfeeding
x1 = dat$M5
x1[x1 == 94] = 0
x1[x1 == 97] = NA
x1[x1 == 98] = NA
x1[x1 == 99] = NA
x1[x1 > 40] = NA                                       # I have problems with x1 again...
# I try to do it using Zambia
xz1 = Zambia$M5
xz1[xz1 == 94] = 0
xz1[xz1 == 97] = NA
xz1[xz1 == 98] = NA
xz1[xz1 == 99] = NA
xz1[xz1 > 40]  = NA

# Variable 2: Age in months of the child
x2 = dat$HW1  # xz2=Zambia$HW1                         # I have no problems (with both of them)  

# Variable 3: Age of the mother at birth
x3 = dat$V012 - dat$B8
x3[x3>45] = NA 
# xz3 = Zambia$V012 - Zambia$B8; x3[xz3>45] = NA       # I have no problems (with both of them)  

# Variable 4: Body mass index (BMI) of the mother
x4 = dat$V445 # xz4 = Zambia$V445                      # I have no problems (with both of them)  

# Variable 5: Height of the respondent in centimeters
x5 = dat$V438
x5[x5 == 9998] = NA
x5[x5 == 9999] = NA
x5[x5 < 1300] = NA
x5[x5 > 1900] = NA
# xz5 = Zambia$V438
# xz5[xz5 == 9998] = NA
# xz5[xz5 == 9999] = NA
# xz5[xz5 < 1300]  = NA
# xz5[xz5 > 1900]  = NA                                # I have no problems (with both of them)  

# Variable 6: Weight of the respondent in kilograms
x6 = dat$V437 # xz6 = Zambia$V437                      # I have no problems (with both of them)  

# Variable 7: De facto region of residence
x7 = as.factor(dat$V101)
x7 = model.matrix(~x7-1)
dim(x7)
# xz7 = as.factor(Zambia$V101)
# xz7 = model.matrix(~xz7-1)
# dim(xz7)                                             # I have no problems (with both of them)  

# Variable 8: Mother highest education level attended
x8 = as.factor(dat$V106)
x8 = model.matrix(~x8-1)
dim(x8)
# xz8 = as.factor(Zambia$V106)
# xz8 = model.matrix(~x8-1)
# dim(xz8)                                             # I have no problems (with both of them)  

# Variable 9: Wealth index factor score
x9 = dat$V191 # xz9 = Zambia$V191
# I have no problems (with both of them)  

# Variable 10: Weight of child at birth given in kilograms with three implied decimal places
x10 = dat$M19
x10[x10 == 9996] = NA
x10[x10 == 9997] = NA
x10[x10 == 9998] = NA
x10[x10 == 9999] = NA
x10 = x10/1000
# I have problems with x1 again...
# I try to do it using Zambia
xz10 = Zambia$M19
xz10[xz10 == 9996] = NA
xz10[xz10 == 9997] = NA
xz10[xz10 == 9998] = NA
xz10[xz10 == 9999] = NA
xz10 = xz10/1000

# Variable 11: Child Sex
x11 = dat$B4 # xz11 = Zambia$B4                         # I have no problems (with both of them)  

# Variable 12: Preceding birth interval is calculated as the difference in months between the current birth and the previous birth
x12 = dat$B11
x12[x12 > 125] = NA
# xz12 = Zambia$B11
# xz12[xz12 > 125] = NA                                 # I have no problems (with both of them)  

# Variable 13: Drinking Water
x13 = dat$V113
x13 = model.matrix(~x13-1)
x13 = x13[,c(2,3,4,8,9,13,17,18)]
mat.sys = na.omit(cbind(y,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13))
dim(mat.sys)[1] 
# I have problems with: dim(mat.sys)[1]=0... no obsevations? 
# I try to do it using Zambia
xz13 = Zambia$V113
xz13 = model.matrix(~xz13-1)
xz13 = xz13[,c(2,3,4,8,9,13,17,18)]                     # I have problems with xz13: "subscript out of bounds"


# MODEL: 
# I consider all Zambia variables excepted xz13 and I try to use x13
zmat.sys = na.omit(cbind(yz,xz1,xz2,xz3,xz4,xz5,xz6,xz7,xz8,xz9,xz10,xz11,xz12,x13))
dim(zmat.sys)[1]             # I use zmat.sys
# Number of regressor
p = dim(zmat.sys)[2];p       # I have a n=1929 per p=32 matrix (I think it appears 32 instead of 13 because    # Construct X and Y          # some covariates have more levels) 
y = zmat.sys[,1]
X = zmat.sys[,2:p]


```

For more information about the libraries, I found on the net:

  + **leaps**:[leaps](https://cran.r-project.org/web/packages/leaps/leaps.pdf)
    - description: leaps() performs an exhaustive search for the best subsets of the variables in x for 
      predicting y in linear regression, using an efficient branch-and-bound algorithm. 
      It is a compatibility wrapper for regsubsets does the same thing better.
      Since the algorithm returns a best model of each size, the results do not depend on a penalty model 
      for model size: it doesn't make any difference whether you want to use AIC, BIC, CIC, DIC, ...


  + **glmulti**:[glmulti](https://cran.r-project.org/web/packages/glmulti/glmulti.pdf)
    - descrition: Automated model selection and model-averaging. Provides a wrapper for glm and other               functions, automatically generating all possible models (under constraints set by the user) with
      the specified response and explanatory variables, and finding the best models in terms of some 
      Information Criterion (AIC,AICc or BIC). Can handle very large numbers of candidate models.
      Features a Genetic Algorithm to find the best models when an exhaustive screening of the candidates 
      is not feasible.


  + **MUMIn**:[MUMIn](https://cran.r-project.org/web/packages/MuMIn/MuMIn.pdf)
    - description: Tools for performing model selection and model averaging. Automated model selection through
      subsetting the maximum model, with optional constraints for model inclusion. 
      Model parameter and prediction averaging based on model weights derived from information criteria
     (AICc and alike) or custom model weighting schemes.


  + **caret**:[caret](https://cran.r-project.org/web/packages/caret/caret.pdf)
    - description: Misc functions for training and plotting classification and regression models.

