Xsub    <-x_train[,which(sub_matrix[j,]==1)]
lm_CV <-lm(y_train~Xsub)
new_Y   <-cbind(rep(1,500),x_test[,which(sub_matrix[j,]==1)])%*% lm_CV$coefficients
cv[j,]      <- t(y_test-new_Y)%*%(y_test-new_Y)
}
BEST_cv<-sub_matrix[which.min(cv),]
BEST_cv[is.na(BEST_cv)] <-0
if(sum(BEST_cv-c(1,1,0,1,0))==0){
Exact[m,2]<-Exact[m,2]+1
}
if(sum((BEST_cv[c(1,2,4)]-c(1,1,1)))==0){
Correct[m,2]<-Correct[m,2]+1
}
AverageN[l,2]<-sum(BEST_cv)
}
Exact
Correct
Average[m,]<-colMeans(AverageN)
}
Exact
Correct
n<-1000
p<-5
Mu<-c(2,4,6,8,10) # location vector
rho<-c(0.2,0.5,0.7)
index_sub_choose<-c(1:p)
sub_matrix <- matrix(data = NA,ncol = p,nrow = 2^p-1)
t=0
for(i in 1:5)
{
index_matrix <- combn(index_sub_choose,i)
for(j in 1:ncol(index_matrix))
{
t <- t+1
index_sub <- index_matrix[,j]
sub_matrix[t,c(index_sub)]  <-  1
}
}
k<-nrow(sub_matrix)
cv <- matrix(data=NA,nrow = k,ncol = 1)
Exact<-data.frame(matrix(0,3,2))
colnames(Exact)<-c("AIC","CV")
rownames(Exact)<-c("r=0.2","r=0.5","r=0.7")
AverageN<-matrix(0,nrow=100,ncol = 2)
colnames(AverageN)<-c("AIC","CV")
Correct<-data.frame(matrix(0,3,2))
colnames(Correct)<-c("AIC","CV")
rownames(Correct)<-c("r=0.2","r=0.5","r=0.7")
Average<-data.frame(matrix(0,3,2))
colnames(Average)<-c("AIC","CV")
rownames(Average)<-c("r=0.2","r=0.5","r=0.7")
for(m in 1:3){
for(l in 1:100){
### SETTING ###
sigma<-matrix(0,p,p)
for(i in 1:p){for(j in 1:p){
sigma[i,j]<-rho[m]^abs(i-j)}}
X<-mvrnorm(n , Mu, sigma)
beta<-c(3,1.5,0,2,0)
e<-rnorm(n, mean = 0, sd = 1)
Y_hat<-X%*%beta+e
### AIC ###
RSS<-rep(0,k)
AIC<-rep(0,k)
for(j in 1:k){
Xsub<-X[,which(sub_matrix[j,]==1)]
lm_AIC<-lm(Y_hat~Xsub)
new_Y   <-cbind(rep(1,500),Xsub)%*% lm_AIC$coefficients
for(i in 1:(n/2)){
RSS[j]<-RSS[j]+(new_Y[i]-Y_hat[i])^2
}
AIC[j]<-RSS[j]/var(e)+2*ncol(data.frame(Xsub))
}
BEST<-sub_matrix[which.min(AIC),]
BEST[is.na(BEST)] <-0
if(sum(BEST-c(1,1,0,1,0))==0){
Exact[m,1]<-Exact[m,1]+1
}
if(sum((BEST[c(1,2,4)]-c(1,1,1)))==0){
Correct[m,1]<-Correct[m,1]+1
}
AverageN[l,1]<-sum(BEST)
### CV ###
index <- sample(1:n, size=0.5*n)
y_train<- Y_hat[-index,]
x_train<-X[-index,]
y_test<- Y_hat[index,]
x_test<-X[index,]
for(j in 1:k){
Xsub    <-x_train[,which(sub_matrix[j,]==1)]
lm_CV <-lm(y_train~Xsub)
new_Y   <-cbind(rep(1,500),x_test[,which(sub_matrix[j,]==1)])%*% lm_CV$coefficients
cv[j,]      <- t(y_test-new_Y)%*%(y_test-new_Y)
}
BEST_cv<-sub_matrix[which.min(cv),]
BEST_cv[is.na(BEST_cv)] <-0
if(sum(BEST_cv-c(1,1,0,1,0))==0){
Exact[m,2]<-Exact[m,2]+1
}
if(sum((BEST_cv[c(1,2,4)]-c(1,1,1)))==0){
Correct[m,2]<-Correct[m,2]+1
}
AverageN[l,2]<-sum(BEST_cv)
}
Average[m,]<-colMeans(AverageN)
}
Exact
Correct
Average
knitr::opts_chunk$set(echo = TRUE)
n<-1000
p<-5
Mu<-c(2,4,6,8,10) # location vector
rho<-c(0.2,0.5,0.7)
index_sub_choose<-c(1:p)
sub_matrix <- matrix(data = NA,ncol = p,nrow = 2^p-1)
t=0
for(i in 1:5)
{
index_matrix <- combn(index_sub_choose,i)
for(j in 1:ncol(index_matrix))
{
t <- t+1
index_sub <- index_matrix[,j]
sub_matrix[t,c(index_sub)]  <-  1
}
}
k<-nrow(sub_matrix)
cv <- matrix(data=NA,nrow = k,ncol = 1)
Exact<-data.frame(matrix(0,3,2))
colnames(Exact)<-c("AIC","CV")
rownames(Exact)<-c("r=0.2","r=0.5","r=0.7")
AverageN<-matrix(0,nrow=100,ncol = 2)
colnames(AverageN)<-c("AIC","CV")
Correct<-data.frame(matrix(0,3,2))
colnames(Correct)<-c("AIC","CV")
rownames(Correct)<-c("r=0.2","r=0.5","r=0.7")
Average<-data.frame(matrix(0,3,2))
colnames(Average)<-c("AIC","CV")
rownames(Average)<-c("r=0.2","r=0.5","r=0.7")
for(m in 1:3){
for(l in 1:100){
### SETTING ###
sigma<-matrix(0,p,p)
for(i in 1:p){for(j in 1:p){
sigma[i,j]<-rho[m]^abs(i-j)}}
X<-mvrnorm(n , Mu, sigma)
beta<-c(3,1.5,0,2,0)
e<-rnorm(n, mean = 0, sd = 1)
Y_hat<-X%*%beta+e
### AIC ###
RSS<-rep(0,k)
AIC<-rep(0,k)
for(j in 1:k){
Xsub<-X[,which(sub_matrix[j,]==1)]
lm_AIC<-lm(Y_hat~Xsub)
new_Y   <-cbind(rep(1,500),Xsub)%*% lm_AIC$coefficients
for(i in 1:(n/2)){
RSS[j]<-RSS[j]+(new_Y[i]-Y_hat[i])^2
}
AIC[j]<-RSS[j]/var(e)+2*ncol(data.frame(Xsub))
}
BEST<-sub_matrix[which.min(AIC),]
BEST[is.na(BEST)] <-0
if(sum(BEST-c(1,1,0,1,0))==0){
Exact[m,1]<-Exact[m,1]+1
}
if(sum((BEST[c(1,2,4)]-c(1,1,1)))==0){
Correct[m,1]<-Correct[m,1]+1
}
AverageN[l,1]<-sum(BEST)
### CV ###
index <- sample(1:n, size=0.5*n)
y_train<- Y_hat[-index,]
x_train<-X[-index,]
y_test<- Y_hat[index,]
x_test<-X[index,]
for(j in 1:k){
Xsub    <-x_train[,which(sub_matrix[j,]==1)]
lm_CV <-lm(y_train~Xsub)
new_Y   <-cbind(rep(1,500),x_test[,which(sub_matrix[j,]==1)])%*% lm_CV$coefficients
cv[j,]      <- t(y_test-new_Y)%*%(y_test-new_Y)
}
BEST_cv<-sub_matrix[which.min(cv),]
BEST_cv[is.na(BEST_cv)] <-0
if(sum(BEST_cv-c(1,1,0,1,0))==0){
Exact[m,2]<-Exact[m,2]+1
}
if(sum((BEST_cv[c(1,2,4)]-c(1,1,1)))==0){
Correct[m,2]<-Correct[m,2]+1
}
AverageN[l,2]<-sum(BEST_cv)
}
Average[m,]<-colMeans(AverageN)
}
Exact
Correct
Average
library(MASS)
library(mvtnorm)
## load the data  ##
load("data_leukemia_reduced.Rda")
data <- data_leukemia_reduced
str(data)
plot(data)
## a ##
## use glm to make a regression ##
glm_1 <- glm(formula = y ~ V980 ,family = binomial,data = data )
summary(glm_1)
## b ##
data_pre <- data.frame(data$V980)
colnames(data_pre)<- "V980"
classify_pre_number <- predict(object = glm_1,newdata = data_pre,type = "response" )
classify_pre_number <- as.data.frame(classify_pre_number)
#Set the cut-of value v980<0.5 is "1"
# here we choose c is euqal to 0.5
c <- 0.5
#Create an empaty matrix to store the data
classify_pre    <-matrix(data = NA,nrow = 72,ncol = 1)
classify_pre[c(which(classify_pre_number<=c)),1]<-0
classify_pre[c(which(classify_pre_number>c)),1]<-1
comp_pre_rea <- cbind(data[,1],classify_pre)
comp_pre_rea <- data.frame(comp_pre_rea)
colnames(comp_pre_rea) <- c("real","predict")
#
t<-table(comp_pre_rea)
tp[i,1] <- t[1,1]/25
#### c ####
## creat the curve ##
tp <- matrix(data = NA,nrow = 100,ncol = 1)
fp <- matrix(data = NA,nrow = 100,ncol = 1)
for(i in 1:100){
c <- i*0.01
#Create an empaty matrix to store the data
classify_pre    <-matrix(data = NA,nrow = 72,ncol = 1)
classify_pre[c(which(classify_pre_number<=c)),1]<-0
classify_pre[c(which(classify_pre_number>c)),1]<-1
comp_pre_rea <- cbind(data[,1],classify_pre)
comp_pre_rea <- data.frame(comp_pre_rea)
colnames(comp_pre_rea) <- c("real","predict")
t<-table(comp_pre_rea)
tp[i,1] <- t[1,1]/25
fp[i,1] <- t[2,1]/47
}
## result of the curve ##
plot(x = fp,y = tp )
## b ##
data_pre <- data.frame(data$V980)
colnames(data_pre)<- "V980"
classify_pre_number <- predict(object = glm_1,newdata = data_pre,type = "response" )
classify_pre_number <- as.data.frame(classify_pre_number)
#Set the cut-of value v980<0.5 is "1"
# here we choose c is euqal to 0.5
c <- 0.5
#Create an empaty matrix to store the data
classify_pre    <-matrix(data = NA,nrow = 72,ncol = 1)
classify_pre[c(which(classify_pre_number<=c)),1]<-0
classify_pre[c(which(classify_pre_number>c)),1]<-1
comp_pre_rea <- cbind(data[,1],classify_pre)
comp_pre_rea <- data.frame(comp_pre_rea)
colnames(comp_pre_rea) <- c("real","predict")
#
t<-table(comp_pre_rea)
tp[i,1] <- t[1,1]/25
np[i,1] <- t[2,1]/47
## b ##
data_pre <- data.frame(data$V980)
colnames(data_pre)<- "V980"
classify_pre_number <- predict(object = glm_1,newdata = data_pre,type = "response" )
classify_pre_number <- as.data.frame(classify_pre_number)
#Set the cut-of value v980<0.5 is "1"
# here we choose c is euqal to 0.5
c <- 0.5
#Create an empaty matrix to store the data
classify_pre    <-matrix(data = NA,nrow = 72,ncol = 1)
classify_pre[c(which(classify_pre_number<=c)),1]<-0
classify_pre[c(which(classify_pre_number>c)),1]<-1
comp_pre_rea <- cbind(data[,1],classify_pre)
comp_pre_rea <- data.frame(comp_pre_rea)
colnames(comp_pre_rea) <- c("real","predict")
#
t<-table(comp_pre_rea)
tp[i,1] <- t[1,1]/25
fp[i,1] <- t[2,1]/47
#### c ####
## creat the curve ##
tp <- matrix(data = NA,nrow = 100,ncol = 1)
fp <- matrix(data = NA,nrow = 100,ncol = 1)
for(i in 1:100){
c <- i*0.01
#Create an empaty matrix to store the data
classify_pre    <-matrix(data = NA,nrow = 72,ncol = 1)
classify_pre[c(which(classify_pre_number<=c)),1]<-0
classify_pre[c(which(classify_pre_number>c)),1]<-1
comp_pre_rea <- cbind(data[,1],classify_pre)
comp_pre_rea <- data.frame(comp_pre_rea)
colnames(comp_pre_rea) <- c("real","predict")
t<-table(comp_pre_rea)
tp[i,1] <- t[1,1]/25
fp[i,1] <- t[2,1]/47
}
## result of the curve ##
plot(x = fp,y = tp )
t
comp_pre_rea
comp_pre_rea
t
tp <- t[1,1]/25
fp <- t[2,1]/47
tp
fp
t<-table(comp_pre_rea)
tp <- t[1,1]/25
fp <- t[2,1]/47
tp
## b ##
data_pre <- data.frame(data$V980)
colnames(data_pre)<- "V980"
classify_pre_number <- predict(object = glm_1,newdata = data_pre,type = "response" )
classify_pre_number <- as.data.frame(classify_pre_number)
#Set the cut-of value v980<0.5 is "1"
# here we choose c is euqal to 0.5
c <- 0.5
#Create an empaty matrix to store the data
classify_pre    <-matrix(data = NA,nrow = 72,ncol = 1)
classify_pre[c(which(classify_pre_number<=c)),1]<-0
classify_pre[c(which(classify_pre_number>c)),1]<-1
comp_pre_rea <- cbind(data[,1],classify_pre)
comp_pre_rea <- data.frame(comp_pre_rea)
colnames(comp_pre_rea) <- c("real","predict")
#
t<-table(comp_pre_rea)
tp <- t[1,1]/25
fp <- t[2,1]/47
comp_pre_rea
View(classify_pre_number)
t<-table(comp_pre_rea)
t
tp <- t[1,1]/25
tp
fp <- t[2,1]/47
fp
install.packages("pROC")
#### c ####
## creat the curve ##
tp <- matrix(data = NA,nrow = 100,ncol = 1)
fp <- matrix(data = NA,nrow = 100,ncol = 1)
for(i in 1:100){
c <- i*0.01
#Create an empaty matrix to store the data
classify_pre    <-matrix(data = NA,nrow = 72,ncol = 1)
classify_pre[c(which(classify_pre_number<=c)),1]<-0
classify_pre[c(which(classify_pre_number>c)),1]<-1
comp_pre_rea <- cbind(data[,1],classify_pre)
comp_pre_rea <- data.frame(comp_pre_rea)
colnames(comp_pre_rea) <- c("real","predict")
t<-table(comp_pre_rea)
tp[i,1] <- t[1,1]/25
fp[i,1] <- t[2,1]/47
}
## result of the curve ##
plot(x = fp,y = tp )
## d ##
library("pROC")
response <- comp_pre_rea[,1]
predictor <- classify_pre
rocobj <- roc(response, predictor) # Very slow!
fp_pack <- 1-rocobj$sensitivities
tp_pack <- rocobj$specificities
#plot from the package
plot(x = fp_pack,y = tp_pack)
## d ##
library("pROC")
response <- comp_pre_rea[,1]
predictor <- comp_pre_rea[,2]
rocobj <- roc(response, predictor) # Very slow!
fp_pack <- 1-rocobj$sensitivities
tp_pack <- rocobj$specificities
#plot from the package
plot(x = fp_pack,y = tp_pack)
plot(x = fp,y = tp )
## load the data  ##
setwd("D:/Gen?ve Universit?/High Dimention Data/practical_4")
## load the data  ##
load("data_leukemia_reduced.Rda")
data <- data_leukemia_reduced
str(data)
plot(data)
## a ##
## use glm to make a regression ##
glm_1 <- glm(formula = y ~ V980 ,family = binomial,data = data )
summary(glm_1)
## b ##
data_pre <- data.frame(data$V980)
colnames(data_pre)<- "V980"
classify_pre_number <- predict(object = glm_1,newdata = data_pre,type = "response" )
classify_pre_number <- as.data.frame(classify_pre_number)
#Set the cut-of value v980<0.5 is "1"
# here we choose c is euqal to 0.5
c <- 0.5
#Create an empaty matrix to store the data
classify_pre    <-matrix(data = NA,nrow = 72,ncol = 1)
classify_pre[c(which(classify_pre_number<=c)),1]<-0
classify_pre[c(which(classify_pre_number>c)),1]<-1
comp_pre_rea <- cbind(data[,1],classify_pre)
comp_pre_rea <- data.frame(comp_pre_rea)
colnames(comp_pre_rea) <- c("real","predict")
#
t<-table(comp_pre_rea)
print("TP")
t[1,1]/25
print("FP")
t[2,1]/47
#### c ####
## creat the curve ##
tp <- matrix(data = NA,nrow = 100,ncol = 1)
fp <- matrix(data = NA,nrow = 100,ncol = 1)
for(i in 1:100){
q <- i*0.01
#Create an empaty matrix to store the data
classify_pre_rp    <-matrix(data = NA,nrow = 72,ncol = 1)
classify_pre_rp[c(which(classify_pre_number<=q)),1]<-0
classify_pre_rp[c(which(classify_pre_number>q)),1]<-1
comp_pre_rea_rp <- cbind(data[,1],classify_pre_rp)
comp_pre_rea_rp <- data.frame(comp_pre_rea_rp)
colnames(comp_pre_rea_rp) <- c("real","predict")
t<-table(comp_pre_rea_rp)
tp[i,1] <- t[1,1]/25
fp[i,1] <- t[2,1]/47
}
## result of the curve ##
plot(x = fp,y = tp )
## d ##
library("pROC", lib.loc="~/R/win-library/3.3")
response <- comp_pre_rea[,1]
predictor <- as.matrix(data_pre)
rocobj <- roc(response, predictor) # Very slow!
fp_pack <- 1-rocobj$sensitivities
tp_pack <- rocobj$specificities
#plot from the package
plot(x = fp_pack,y = tp_pack)
plot(x = fp,y = tp )
setwd("C:/Users/user/Desktop/Spring2018/MachineLearning")
load("C:/Users/user/Desktop/Spring2018/MachineLearning/classSim.RData")
install.packages("class")
load("ClassSim")
load("ClassSim.RData")
library(class)
help(knn)
index<-sample(nrow(ClassSim),0.5*nrow(ClassSim),replace=FALSE)
ClassSim
ClassSim<-ClassSim
index<-sample(nrow(classSim),0.5*nrow(classSim),replace=FALSE)
Train<-classSim[,index]
train<-classSim[index,]
test<-classSim[-index,]
train
knn(train,test,train[,3],k=1)
knn(train,test,train[,3],k=1,prob = TRUE)
knn(train,test,train[,3],k=1)
train[,3]==knn(train,test,train[,3],k=1)
mean(train[,3]==knn(train,test,train[,3],k=1))
mean(test[,3]==knn(train,test,train[,3],k=1))
mean(test[,3]==knn(train[,1:2],test[,1:2],train[,3],k=1))
mean(test[,3]==knn(train[,1:2],test[,1:2],train[,3],k=1))
knn(train[,1:2],train[,3],k=1)
mean(train[,3]==knn(train[,1:2],train[,1:2],train[,3],k=1))
mean(train[,3]==knn(train[,1:2],train[,1:2],train[,3],k=2))
mean(train[,3]==knn(train[,1:2],train[,1:2],train[,3],k=3))
mean(train[,3]==knn(train[,1:2],train[,1:2],train[,3],k=4))
mean((train[,3]-knn(train[,1:2],train[,1:2],train[,3],k=4))^2)
(train[,3]-knn(train[,1:2],train[,1:2],train[,3],k=4))^2
knn(train[,1:2],train[,1:2],train[,3],k=4)
(train[,3]-as.vector(knn(train[,1:2],train[,1:2],train[,3],k=4))^2)
as.vector(knn(train[,1:2],train[,1:2],train[,3],k=4)
(train[,3]-as.vector(knn(train[,1:2],train[,1:2],train[,3],k=4))
as.vector(knn(train[,1:2],train[,1:2],train[,3],k=4))
as.vector(knn(train[,1:2],train[,1:2],train[,3],k=4))
(train[,3]-as.numeric(knn(train[,1:2],train[,1:2],train[,3],k=4))^2)
(train[,3]-as.numeric(as.factor(knn(train[,1:2],train[,1:2],train[,3],k=4))^2)
(train[,3]-as.numeric(as.factor(knn(train[,1:2],train[,1:2],train[,3],k=4)))^2)
(train[,3]-as.numeric(as.factor(knn(train[,1:2],train[,1:2],train[,3],k=4)))^2)
)
(train[,3]-as.numeric(as.factor(knn(train[,1:2],train[,1:2],train[,3],k=4)))^2)
knn(train[,1:2],train[,1:2],train[,3],k=4)
as.numeric(as.charactor(knn(train[,1:2],train[,1:2],train[,3],k=4)))
as.numeric(as.character(knn(train[,1:2],train[,1:2],train[,3],k=4)))
mean(train[,3]-as.numeric(as.character(knn(train[,1:2],train[,1:2],train[,3],k=4))))^2
train_error<-rep(0,50)
for(k in 1:50){
train_error[i]<-mean(train[,3]-as.numeric(as.character(knn(train[,1:2],train[,1:2],train[,3],k=i))))^2
}
train_error<-rep(0,50)
for(k in 1:50){
train_error[i]<-mean(train[,3]-as.numeric(as.character(knn(train[,1:2],train[,1:2],train[,3],k))))^2
}
train_error<-rep(0,50)
for(k in 1:50){
train_error[k]<-mean(train[,3]-as.numeric(as.character(knn(train[,1:2],train[,1:2],train[,3],k))))^2
}
train_error
cbind(c(1:50),train_error)
plot(cbind(c(1:50),train_error))
train_error[k]<-mean((train[,3]-as.numeric(as.character(knn(train[,1:2],train[,1:2],train[,3],k))))^2)
train_error<-rep(0,50)
for(k in 1:50){
train_error[k]<-mean((train[,3]-as.numeric(as.character(knn(train[,1:2],train[,1:2],train[,3],k))))^2)
}
plot(cbind(c(1:50),train_error))
train_error
plot(cbind(c(1:50),train_error))
plot(c(1:50),train_error)
